# -*- coding: utf-8 -*-
"""credit-card-fraud-detection-final-assignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gfe54S6TOgMqSK1wyPiehSn7jejSl7kc

# Import necessary libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, classification_report, precision_recall_curve, auc, roc_curve, accuracy_score, precision_score, recall_score, f1_score
import warnings
warnings.filterwarnings('ignore')

"""# Set random seed for reproducibility"""

np.random.seed(42)

"""## Data Collection"""

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/CreditCardFraudDetection/creditcard.csv')

"""## Data Preprocessing and Exploration

## Check the shape of the dataset
"""

print(f"Dataset shape: {df.shape}")

"""## Display the first few rows"""

print("\nFirst 5 rows:")
print(df.head())

"""## Check for missing values"""

print("\nMissing values:")
print(df.isnull().sum())

"""# Check for duplicates"""

print(f"\nDuplicate rows: {df.duplicated().sum()}")

"""## Dataset information"""

print("\nDataset info:")
print(df.info())

"""## Summary statistics"""

print(df.describe())

"""## Class distribution (0: Normal, 1: Fraud)"""

fraud_percent = df['Class'].value_counts()[1] / df.shape[0] * 100
print(f"Percentage of fraudulent transactions: {fraud_percent:.4f}%")

plt.figure(figsize=(10, 6))
sns.countplot(x='Class', data=df)
plt.title('Distribution of Normal vs Fraudulent Transactions')
plt.xlabel('Class (0: Normal, 1: Fraud)')
plt.ylabel('Count')
plt.show()

"""## Plot the distribution of transaction amounts"""

plt.figure(figsize=(12, 6))

plt.subplot(1, 2, 1)
sns.histplot(df[df['Class'] == 0]['Amount'], bins=50, kde=True)
plt.title('Transaction Amounts for Normal Transactions')
plt.xlabel('Amount')
plt.xlim([0, 500])

plt.subplot(1, 2, 2)
sns.histplot(df[df['Class'] == 1]['Amount'], bins=50, kde=True, color='red')
plt.title('Transaction Amounts for Fraudulent Transactions')
plt.xlabel('Amount')
plt.xlim([0, 500])

plt.tight_layout()
plt.show()

"""## Time distribution of transactions"""

plt.figure(figsize=(14, 6))

plt.subplot(1, 2, 1)
sns.histplot(df[df['Class'] == 0]['Time'], bins=50, kde=True)
plt.title('Time Distribution for Normal Transactions')
plt.xlabel('Time (seconds)')

plt.subplot(1, 2, 2)
sns.histplot(df[df['Class'] == 1]['Time'], bins=50, kde=True, color='red')
plt.title('Time Distribution for Fraudulent Transactions')
plt.xlabel('Time (seconds)')

plt.tight_layout()
plt.show()

"""## Correlation matrix of features"""

plt.figure(figsize=(16, 14))
corr_matrix = df.corr()
mask = np.triu(np.ones_like(corr_matrix, dtype=bool))
sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix of Features')
plt.show()

"""## Look at correlations with the target variable"""

plt.figure(figsize=(12, 10))
correlations = df.corr()['Class'].sort_values(ascending=False)
sns.barplot(x=correlations.index, y=correlations.values)
plt.title('Correlation of Features with Target Class')
plt.xticks(rotation=90)
plt.show()

"""### Feature Engineering and Preprocessing"""

# Normalize 'Amount' and 'Time' features
df['Amount_norm'] = StandardScaler().fit_transform(df['Amount'].values.reshape(-1, 1))
df['Time_norm'] = StandardScaler().fit_transform(df['Time'].values.reshape(-1, 1))

# Drop the original 'Amount' and 'Time' columns
df = df.drop(['Amount', 'Time'], axis=1)

# Prepare the data for modeling
X = df.drop('Class', axis=1)
y = df['Class']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

print(f"Training set shape: {X_train.shape}")
print(f"Testing set shape: {X_test.shape}")
print(f"Frauds in training set: {sum(y_train == 1)}")
print(f"Frauds in testing set: {sum(y_test == 1)}")

"""## Building and Evaluating Models
### Logistic Regression Model
"""

# Train a logistic regression model
lr_model = LogisticRegression(max_iter=1000, class_weight='balanced')
lr_model.fit(X_train, y_train)

# Make predictions
y_pred_lr = lr_model.predict(X_test)
y_pred_prob_lr = lr_model.predict_proba(X_test)[:, 1]

# Evaluate the model
print("Logistic Regression Model Evaluation:")
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_lr))

print("\nClassification Report:")
print(classification_report(y_test, y_pred_lr))

# Calculate metrics
accuracy_lr = accuracy_score(y_test, y_pred_lr)
precision_lr = precision_score(y_test, y_pred_lr)
recall_lr = recall_score(y_test, y_pred_lr)
f1_lr = f1_score(y_test, y_pred_lr)

print(f"Accuracy: {accuracy_lr:.4f}")
print(f"Precision: {precision_lr:.4f}")
print(f"Recall: {recall_lr:.4f}")
print(f"F1 Score: {f1_lr:.4f}")

"""## ROC Curve"""

fpr_lr, tpr_lr, _ = roc_curve(y_test, y_pred_prob_lr)
roc_auc_lr = auc(fpr_lr, tpr_lr)

plt.figure(figsize=(10, 8))
plt.plot(fpr_lr, tpr_lr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc_lr:.2f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) - Logistic Regression')
plt.legend(loc="lower right")
plt.show()

"""## Precision-Recall Curve"""

precision_lr_curve, recall_lr_curve, _ = precision_recall_curve(y_test, y_pred_prob_lr)
pr_auc_lr = auc(recall_lr_curve, precision_lr_curve)

plt.figure(figsize=(10, 8))
plt.plot(recall_lr_curve, precision_lr_curve, color='blue', lw=2, label=f'PR curve (area = {pr_auc_lr:.2f})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - Logistic Regression')
plt.legend(loc="lower left")
plt.show()

"""## Random Forest Model"""

rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')
rf_model.fit(X_train, y_train)

"""### Make predictions & Evaluate the model"""

y_pred_rf = rf_model.predict(X_test)
y_pred_prob_rf = rf_model.predict_proba(X_test)[:, 1]

print("Random Forest Model Evaluation:")
print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred_rf))

print("\nClassification Report:")
print(classification_report(y_test, y_pred_rf))

"""### Calculate metrics"""

accuracy_rf = accuracy_score(y_test, y_pred_rf)
precision_rf = precision_score(y_test, y_pred_rf)
recall_rf = recall_score(y_test, y_pred_rf)
f1_rf = f1_score(y_test, y_pred_rf)

print(f"Accuracy: {accuracy_rf:.4f}")
print(f"Precision: {precision_rf:.4f}")
print(f"Recall: {recall_rf:.4f}")
print(f"F1 Score: {f1_rf:.4f}")

"""## ROC Curve"""

fpr_rf, tpr_rf, _ = roc_curve(y_test, y_pred_prob_rf)
roc_auc_rf = auc(fpr_rf, tpr_rf)

plt.figure(figsize=(10, 8))
plt.plot(fpr_rf, tpr_rf, color='green', lw=2, label=f'ROC curve (area = {roc_auc_rf:.2f})')
plt.plot([0, 1], [0, 1], color='gray', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) - Random Forest')
plt.legend(loc="lower right")
plt.show()

"""## Precision-Recall Curve"""

precision_rf_curve, recall_rf_curve, _ = precision_recall_curve(y_test, y_pred_prob_rf)
pr_auc_rf = auc(recall_rf_curve, precision_rf_curve)

plt.figure(figsize=(10, 8))
plt.plot(recall_rf_curve, precision_rf_curve, color='green', lw=2, label=f'PR curve (area = {pr_auc_rf:.2f})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve - Random Forest')
plt.legend(loc="lower left")
plt.show()

"""## Feature importance"""

feature_importance = pd.DataFrame({
    'Feature': X.columns,
    'Importance': rf_model.feature_importances_
}).sort_values('Importance', ascending=False)

plt.figure(figsize=(12, 8))
sns.barplot(x='Importance', y='Feature', data=feature_importance.head(15))
plt.title('Top 15 Feature Importances - Random Forest')
plt.tight_layout()
plt.show()

"""## Compare Models

### Compare model performance
"""

models = ['Logistic Regression', 'Random Forest']
accuracy_scores = [accuracy_lr, accuracy_rf]
precision_scores = [precision_lr, precision_rf]
recall_scores = [recall_lr, recall_rf]
f1_scores = [f1_lr, f1_rf]
roc_auc_scores = [roc_auc_lr, roc_auc_rf]
pr_auc_scores = [pr_auc_lr, pr_auc_rf]

comparison_df = pd.DataFrame({
    'Model': models,
    'Accuracy': accuracy_scores,
    'Precision': precision_scores,
    'Recall': recall_scores,
    'F1 Score': f1_scores,
    'ROC AUC': roc_auc_scores,
    'PR AUC': pr_auc_scores
})

print("Model Comparison:")
print(comparison_df)

"""### Plot the comparison"""

plt.figure(figsize=(15, 10))

plt.subplot(2, 2, 1)
sns.barplot(x='Model', y='Accuracy', data=comparison_df)
plt.title('Accuracy Comparison')
plt.ylim(0.9, 1.0)

plt.subplot(2, 2, 2)
sns.barplot(x='Model', y='Precision', data=comparison_df)
plt.title('Precision Comparison')
plt.ylim(0.7, 1.0)

plt.subplot(2, 2, 3)
sns.barplot(x='Model', y='Recall', data=comparison_df)
plt.title('Recall Comparison')
plt.ylim(0.7, 1.0)

plt.subplot(2, 2, 4)
sns.barplot(x='Model', y='F1 Score', data=comparison_df)
plt.title('F1 Score Comparison')
plt.ylim(0.7, 1.0)

plt.tight_layout()
plt.show()

"""### ROC Curve comparison"""

plt.figure(figsize=(10, 8))
plt.plot(fpr_lr, tpr_lr, 'b-', label=f'Logistic Regression (area = {roc_auc_lr:.2f})')
plt.plot(fpr_rf, tpr_rf, 'g-', label=f'Random Forest (area = {roc_auc_rf:.2f})')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend(loc="lower right")
plt.show()

"""### Precision-Recall Curve comparison"""

plt.figure(figsize=(10, 8))
plt.plot(recall_lr_curve, precision_lr_curve, 'b-', label=f'Logistic Regression (area = {pr_auc_lr:.2f})')
plt.plot(recall_rf_curve, precision_rf_curve, 'g-', label=f'Random Forest (area = {pr_auc_rf:.2f})')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('Recall')
plt.ylabel('Precision')
plt.title('Precision-Recall Curve Comparison')
plt.legend(loc="lower left")
plt.show()

"""## Conclusion"""

best_model = comparison_df.iloc[comparison_df['F1 Score'].argmax()]['Model']
best_f1 = comparison_df['F1 Score'].max()

print(f"Based on F1 Score, the best performing model is {best_model} with a score of {best_f1:.4f}.")